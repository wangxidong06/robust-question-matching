{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2021 CCF BDCI 千言-问题匹配鲁棒性评测Baseline\n",
    "\n",
    "\n",
    "**[2021 CCF BDCI 千言-问题匹配鲁棒性评测比赛👈](https://aistudio.baidu.com/aistudio/competition/detail/116/0/introduction)**\n",
    "\n",
    "**[Baseline Github👈](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/text_matching/question_matching)**\n",
    "\n",
    "本案例介绍 NLP 最基本的任务类型之一 —— 文本语义匹配，并且基于 PaddleNLP 使用百度开源的预训练模型 ERNIE-Gram 搭建效果优异的语义匹配模型，来判断 2 段文本语义是否相同。\n",
    "\n",
    "## 1. 背景介绍\n",
    "文本语义匹配任务，简单来说就是给定两段文本，让模型来判断两段文本是不是语义相似。\n",
    "\n",
    "以权威的语义匹配数据集 [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) 为例，[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) 数据集是基于百度知道相似问题推荐构造的通问句语义匹配数据集。训练集中的每两段文本都会被标记为 1（语义相似） 或者 0（语义不相似）。更多数据集可访问[千言](https://www.luge.ai/)获取哦。\n",
    "\n",
    "例如百度知道场景下，用户搜索一个问题，模型会计算这个问题与候选问题是否语义相似，语义匹配模型会找出与问题语义相似的候选问题返回给用户，加快用户提问-获取答案的效率。例如，当某用户在搜索引擎中搜索 “深度学习的教材有哪些？”，模型就自动找到了一些语义相似的问题展现给用户:\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/ecc1244685ec4476b869ce8a32d421c0ad530666e98d487da21fa4f61670544f\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.快速实践\n",
    "\n",
    "介绍如何准备数据，基于 ERNIE-Gram 模型搭建匹配网络，然后快速进行语义匹配模型的训练、评估和预测。\n",
    "\n",
    "### 2.1 数据加载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already up-to-date: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: paddlefsl==1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl==1.0.0->paddlenlp) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: requests~=2.24.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl==1.0.0->paddlenlp) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm~=4.27.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl==1.0.0->paddlenlp) (4.27.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow==8.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl==1.0.0->paddlenlp) (8.2.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "# 正式开始实验之前首先通过如下命令安装最新版本的 paddlenlp\n",
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data\r\n",
      "├── data104940\r\n",
      "│   └── train.zip\r\n",
      "├── data104941\r\n",
      "│   └── test_A\r\n",
      "├── __MACOSX\r\n",
      "│   └── train\r\n",
      "│       ├── BQ\r\n",
      "│       ├── LCQMC\r\n",
      "│       └── OPPO\r\n",
      "└── train\r\n",
      "    ├── BQ\r\n",
      "    │   ├── dev\r\n",
      "    │   ├── test\r\n",
      "    │   └── train\r\n",
      "    ├── LCQMC\r\n",
      "    │   ├── dev\r\n",
      "    │   ├── test\r\n",
      "    │   └── train\r\n",
      "    └── OPPO\r\n",
      "        ├── dev\r\n",
      "        └── train\r\n",
      "\r\n",
      "11 directories, 10 files\r\n"
     ]
    }
   ],
   "source": [
    "# 检查数据集所在路径\n",
    "!tree -L 3 /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/data104940/train.zip\n",
      "  inflating: data/train/.DS_Store    \n",
      "  inflating: data/__MACOSX/train/._.DS_Store  \n",
      "  inflating: data/train/BQ/test      \n",
      "  inflating: data/__MACOSX/train/BQ/._test  \n",
      "  inflating: data/train/BQ/train     \n",
      "  inflating: data/__MACOSX/train/BQ/._train  \n",
      "  inflating: data/train/BQ/dev       \n",
      "  inflating: data/__MACOSX/train/BQ/._dev  \n",
      "  inflating: data/__MACOSX/train/._BQ  \n",
      "  inflating: data/train/LCQMC/test   \n",
      "  inflating: data/__MACOSX/train/LCQMC/._test  \n",
      "  inflating: data/train/LCQMC/train  \n",
      "  inflating: data/__MACOSX/train/LCQMC/._train  \n",
      "  inflating: data/train/LCQMC/dev    \n",
      "  inflating: data/__MACOSX/train/LCQMC/._dev  \n",
      "  inflating: data/__MACOSX/train/._LCQMC  \n",
      "  inflating: data/train/OPPO/train   \n",
      "  inflating: data/__MACOSX/train/OPPO/._train  \n",
      "  inflating: data/train/OPPO/dev     \n",
      "  inflating: data/__MACOSX/train/OPPO/._dev  \n",
      "  inflating: data/__MACOSX/train/._OPPO  \n",
      "  inflating: data/__MACOSX/._train   \n"
     ]
    }
   ],
   "source": [
    "!unzip -o data/data104940/train.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将LCQMC、BQ、OPPO三个数据集的训练集和验证集合并\n",
    "!cat ./data/train/LCQMC/train ./data/train/BQ/train ./data/train/OPPO/train > train.txt\n",
    "!cat ./data/train/LCQMC/dev ./data/train/BQ/dev ./data/train/OPPO/dev > dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "from work.data import create_dataloader, read_text_pair, convert_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset(read_text_pair, data_path=\"train.txt\", is_test=False, lazy=False)\n",
    "\n",
    "dev_ds = load_dataset(read_text_pair, data_path=\"dev.txt\", is_test=False, lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query1': '喜欢打篮球的男生喜欢什么样的女生', 'query2': '爱打篮球的男生喜欢什么样的女生', 'label': '1'}\n",
      "{'query1': '我手机丢了，我想换个手机', 'query2': '我想买个新手机，求推荐', 'label': '1'}\n",
      "{'query1': '大家觉得她好看吗', 'query2': '大家觉得跑男好看吗？', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "# 输出训练集的前 3 条样本\n",
    "for idx, example in enumerate(train_ds):\n",
    "    if idx <= 2:\n",
    "        print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 数据预处理\n",
    "\n",
    "通过 PaddleNLP 加载进来的数据集是原始的明文数据集，这部分我们来实现组 batch、tokenize 等预处理逻辑，将原始明文数据转换成网络训练的输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-24 23:48:10,370] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# 因为是基于预训练模型 ERNIE-Gram 来进行，所以需要首先加载 ERNIE-Gram 的 tokenizer，\n",
    "# 后续样本转换函数基于 tokenizer 对文本进行切分\n",
    "tokenizer = ppnlp.transformers.ErnieGramTokenizer.from_pretrained('ernie-gram-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 对训练集的第 1 条数据进行转换\n",
    "input_ids, token_type_ids, label = convert_example(train_ds[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 692, 811, 445, 2001, 497, 5, 654, 21, 692, 811, 614, 356, 314, 5, 291, 21, 2, 329, 445, 2001, 497, 5, 654, 21, 692, 811, 614, 356, 314, 5, 291, 21, 2]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 为了后续方便使用，我们使用python偏函数（partial）给 convert_example 赋予一些默认参数\n",
    "# 训练集和验证集的样本转换函数\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, max_seq_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 我们的训练数据会返回 input_ids, token_type_ids, labels 3 个字段\n",
    "# 因此针对这 3 个字段需要分别定义 3 个组 batch 操作\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # text_pair_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # text_pair_segment\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 定义 Dataloader\n",
    "下面我们基于组 batchify_fn 函数和样本转换函数 trans_func 来构造训练集的 DataLoader, 支持多卡训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=32,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "dev_data_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        mode='dev',\n",
    "        batch_size=128,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 模型搭建\n",
    "\n",
    "自从 2018 年 10 月以来，NLP 个领域的任务都通过 Pretrain + Finetune 的模式相比传统 DNN 方法在效果上取得了显著的提升，本节我们以百度开源的预训练模型 ERNIE-Gram 为基础模型，在此之上构建 Point-wise 语义匹配网络。\n",
    "\n",
    "首先我们来定义网络结构:\n",
    "\n",
    "- Point-wise 语义匹配网络\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/e5415a9928064f299e94f91fcc0e4ce6edc98cf718c44c66a5084e672ae6ef78\" width=\"600\" />\n",
    "\n",
    "- ERNIE-Gram\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/7a9c14983bbe4236ad90bcc45258acfa32704668549541909947803be338e2a3\" width=\"600\" />\n",
    "\n",
    "- Rdrop\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d09ce0ffecfe4c849366c5a3ce04e8c2b266a92cba294df38a3c4700d313be18\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-24 23:48:33,426] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/ernie_gram_zh.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 我们基于 ERNIE-Gram 模型结构搭建 Point-wise 语义匹配网络\n",
    "# 所以此处先定义 ERNIE-Gram 的 pretrained_model\n",
    "pretrained_model = ppnlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
    "\n",
    "class QuestionMatching(nn.Layer):\n",
    "    def __init__(self, pretrained_model, dropout=None, rdrop_coef=0.0):\n",
    "        super().__init__()\n",
    "        self.ptm = pretrained_model\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "\n",
    "        # num_labels = 2 (similar or dissimilar)\n",
    "        self.classifier = nn.Linear(self.ptm.config[\"hidden_size\"], 2)\n",
    "        self.rdrop_coef = rdrop_coef\n",
    "        self.rdrop_loss = ppnlp.losses.RDropLoss()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                attention_mask=None,\n",
    "                do_evaluate=False):\n",
    "\n",
    "        _, cls_embedding1 = self.ptm(input_ids, token_type_ids, position_ids, attention_mask)\n",
    "        cls_embedding1 = self.dropout(cls_embedding1)\n",
    "        logits1 = self.classifier(cls_embedding1)\n",
    "        \n",
    "        # For more information about R-drop please refer to this paper: https://arxiv.org/abs/2106.14448\n",
    "        # Original implementation please refer to this code: https://github.com/dropreg/R-Drop\n",
    "        if self.rdrop_coef > 0 and not do_evaluate:\n",
    "            _, cls_embedding2 = self.ptm(input_ids, token_type_ids, position_ids, attention_mask)\n",
    "            cls_embedding2 = self.dropout(cls_embedding2)\n",
    "            logits2 = self.classifier(cls_embedding2)\n",
    "            kl_loss = self.rdrop_loss(logits1, logits2)\n",
    "        else:\n",
    "            kl_loss = 0.0\n",
    "\n",
    "        return logits1, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = QuestionMatching(pretrained_model, rdrop_coef=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 模型训练 & 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "lr_scheduler = LinearDecayWithWarmup(5e-5, num_training_steps, 0.0)\n",
    "\n",
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    \n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=0.0,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    total_num = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        total_num += len(labels)\n",
    "        logits, _ = model(input_ids=input_ids, token_type_ids=token_type_ids, do_evaluate=True)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "\n",
    "    print(\"dev_loss: {:.5}, accuracy: {:.5}, total_num:{}\".format(np.mean(losses), accu, total_num))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.6175, ce_loss: 0.6175., kl_loss: 0.0000, accu: 0.5813, speed: 7.59 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.5776, ce_loss: 0.5776., kl_loss: 0.0000, accu: 0.6000, speed: 9.83 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.5113, ce_loss: 0.5113., kl_loss: 0.0000, accu: 0.6260, speed: 9.80 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.5664, ce_loss: 0.5664., kl_loss: 0.0000, accu: 0.6461, speed: 9.78 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.6439, ce_loss: 0.6439., kl_loss: 0.0000, accu: 0.6600, speed: 11.01 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.6244, ce_loss: 0.6244., kl_loss: 0.0000, accu: 0.6687, speed: 9.87 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.3942, ce_loss: 0.3942., kl_loss: 0.0000, accu: 0.6857, speed: 9.60 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.6121, ce_loss: 0.6121., kl_loss: 0.0000, accu: 0.6918, speed: 10.21 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.3481, ce_loss: 0.3481., kl_loss: 0.0000, accu: 0.6990, speed: 10.77 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.5572, ce_loss: 0.5572., kl_loss: 0.0000, accu: 0.7016, speed: 9.63 step/s\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.5894, ce_loss: 0.5894., kl_loss: 0.0000, accu: 0.7051, speed: 10.13 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.5046, ce_loss: 0.5046., kl_loss: 0.0000, accu: 0.7107, speed: 10.26 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.5550, ce_loss: 0.5550., kl_loss: 0.0000, accu: 0.7163, speed: 9.12 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.3854, ce_loss: 0.3854., kl_loss: 0.0000, accu: 0.7172, speed: 9.84 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.6192, ce_loss: 0.6192., kl_loss: 0.0000, accu: 0.7210, speed: 9.38 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.4645, ce_loss: 0.4645., kl_loss: 0.0000, accu: 0.7252, speed: 9.53 step/s\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.3939, ce_loss: 0.3939., kl_loss: 0.0000, accu: 0.7278, speed: 10.46 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.5798, ce_loss: 0.5798., kl_loss: 0.0000, accu: 0.7321, speed: 11.09 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.5599, ce_loss: 0.5599., kl_loss: 0.0000, accu: 0.7329, speed: 10.26 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.3266, ce_loss: 0.3266., kl_loss: 0.0000, accu: 0.7362, speed: 10.39 step/s\n",
      "dev_loss: 0.49613, accuracy: 0.77009, total_num:28802\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.4479, ce_loss: 0.4479., kl_loss: 0.0000, accu: 0.7937, speed: 0.29 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.5980, ce_loss: 0.5980., kl_loss: 0.0000, accu: 0.8094, speed: 10.63 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.4304, ce_loss: 0.4304., kl_loss: 0.0000, accu: 0.8073, speed: 10.11 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.2967, ce_loss: 0.2967., kl_loss: 0.0000, accu: 0.8031, speed: 9.93 step/s\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.4559, ce_loss: 0.4559., kl_loss: 0.0000, accu: 0.7975, speed: 9.87 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.5846, ce_loss: 0.5846., kl_loss: 0.0000, accu: 0.7927, speed: 9.39 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.5715, ce_loss: 0.5715., kl_loss: 0.0000, accu: 0.7920, speed: 9.64 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.4105, ce_loss: 0.4105., kl_loss: 0.0000, accu: 0.7922, speed: 10.14 step/s\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.5934, ce_loss: 0.5934., kl_loss: 0.0000, accu: 0.7889, speed: 9.38 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.5936, ce_loss: 0.5936., kl_loss: 0.0000, accu: 0.7887, speed: 9.93 step/s\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.2517, ce_loss: 0.2517., kl_loss: 0.0000, accu: 0.7889, speed: 9.35 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.5683, ce_loss: 0.5683., kl_loss: 0.0000, accu: 0.7878, speed: 10.49 step/s\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.3709, ce_loss: 0.3709., kl_loss: 0.0000, accu: 0.7894, speed: 10.41 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.5695, ce_loss: 0.5695., kl_loss: 0.0000, accu: 0.7931, speed: 9.72 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.5525, ce_loss: 0.5525., kl_loss: 0.0000, accu: 0.7931, speed: 9.82 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.4392, ce_loss: 0.4392., kl_loss: 0.0000, accu: 0.7900, speed: 9.61 step/s\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.3646, ce_loss: 0.3646., kl_loss: 0.0000, accu: 0.7893, speed: 10.28 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.4094, ce_loss: 0.4094., kl_loss: 0.0000, accu: 0.7903, speed: 11.31 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.5652, ce_loss: 0.5652., kl_loss: 0.0000, accu: 0.7885, speed: 11.08 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.4017, ce_loss: 0.4017., kl_loss: 0.0000, accu: 0.7889, speed: 9.25 step/s\n",
      "dev_loss: 0.54604, accuracy: 0.75745, total_num:28802\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.6149, ce_loss: 0.6149., kl_loss: 0.0000, accu: 0.7844, speed: 0.30 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.2831, ce_loss: 0.2831., kl_loss: 0.0000, accu: 0.7891, speed: 9.89 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.5227, ce_loss: 0.5227., kl_loss: 0.0000, accu: 0.7833, speed: 9.92 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.3020, ce_loss: 0.3020., kl_loss: 0.0000, accu: 0.7875, speed: 9.94 step/s\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.4327, ce_loss: 0.4327., kl_loss: 0.0000, accu: 0.8000, speed: 9.89 step/s\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.5397, ce_loss: 0.5397., kl_loss: 0.0000, accu: 0.8016, speed: 10.31 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.6421, ce_loss: 0.6421., kl_loss: 0.0000, accu: 0.8013, speed: 10.12 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.3306, ce_loss: 0.3306., kl_loss: 0.0000, accu: 0.8059, speed: 9.93 step/s\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.5526, ce_loss: 0.5526., kl_loss: 0.0000, accu: 0.8063, speed: 9.56 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.7031, ce_loss: 0.7031., kl_loss: 0.0000, accu: 0.8019, speed: 10.19 step/s\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.3049, ce_loss: 0.3049., kl_loss: 0.0000, accu: 0.8017, speed: 10.53 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.4084, ce_loss: 0.4084., kl_loss: 0.0000, accu: 0.8031, speed: 10.28 step/s\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.3958, ce_loss: 0.3958., kl_loss: 0.0000, accu: 0.8012, speed: 10.55 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.7291, ce_loss: 0.7291., kl_loss: 0.0000, accu: 0.7991, speed: 9.46 step/s\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.5164, ce_loss: 0.5164., kl_loss: 0.0000, accu: 0.7987, speed: 10.02 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.5365, ce_loss: 0.5365., kl_loss: 0.0000, accu: 0.7975, speed: 9.71 step/s\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.3956, ce_loss: 0.3956., kl_loss: 0.0000, accu: 0.7998, speed: 8.93 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.2960, ce_loss: 0.2960., kl_loss: 0.0000, accu: 0.8014, speed: 10.06 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.3228, ce_loss: 0.3228., kl_loss: 0.0000, accu: 0.8007, speed: 10.52 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.4265, ce_loss: 0.4265., kl_loss: 0.0000, accu: 0.7983, speed: 10.36 step/s\n",
      "dev_loss: 0.45913, accuracy: 0.7896, total_num:28802\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.2518, ce_loss: 0.2518., kl_loss: 0.0000, accu: 0.8031, speed: 0.29 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.2792, ce_loss: 0.2792., kl_loss: 0.0000, accu: 0.8078, speed: 10.60 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.5155, ce_loss: 0.5155., kl_loss: 0.0000, accu: 0.8146, speed: 10.53 step/s\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.3014, ce_loss: 0.3014., kl_loss: 0.0000, accu: 0.8141, speed: 10.16 step/s\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.8349, ce_loss: 0.8349., kl_loss: 0.0000, accu: 0.8144, speed: 10.29 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.4453, ce_loss: 0.4453., kl_loss: 0.0000, accu: 0.8104, speed: 9.97 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.4124, ce_loss: 0.4124., kl_loss: 0.0000, accu: 0.8094, speed: 9.95 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.5378, ce_loss: 0.5378., kl_loss: 0.0000, accu: 0.8121, speed: 9.65 step/s\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.3940, ce_loss: 0.3940., kl_loss: 0.0000, accu: 0.8069, speed: 10.70 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.3961, ce_loss: 0.3961., kl_loss: 0.0000, accu: 0.8094, speed: 11.40 step/s\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.2661, ce_loss: 0.2661., kl_loss: 0.0000, accu: 0.8099, speed: 10.93 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.5099, ce_loss: 0.5099., kl_loss: 0.0000, accu: 0.8094, speed: 9.72 step/s\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.5040, ce_loss: 0.5040., kl_loss: 0.0000, accu: 0.8084, speed: 10.42 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.3128, ce_loss: 0.3128., kl_loss: 0.0000, accu: 0.8105, speed: 9.70 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.2652, ce_loss: 0.2652., kl_loss: 0.0000, accu: 0.8079, speed: 9.58 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.5191, ce_loss: 0.5191., kl_loss: 0.0000, accu: 0.8102, speed: 9.61 step/s\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.4106, ce_loss: 0.4106., kl_loss: 0.0000, accu: 0.8118, speed: 9.20 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.6223, ce_loss: 0.6223., kl_loss: 0.0000, accu: 0.8116, speed: 9.52 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.4708, ce_loss: 0.4708., kl_loss: 0.0000, accu: 0.8141, speed: 9.86 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.4898, ce_loss: 0.4898., kl_loss: 0.0000, accu: 0.8141, speed: 9.86 step/s\n",
      "dev_loss: 0.46854, accuracy: 0.78321, total_num:28802\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.4737, ce_loss: 0.4737., kl_loss: 0.0000, accu: 0.8094, speed: 0.30 step/s\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.4140, ce_loss: 0.4140., kl_loss: 0.0000, accu: 0.8078, speed: 8.97 step/s\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.4691, ce_loss: 0.4691., kl_loss: 0.0000, accu: 0.8052, speed: 10.00 step/s\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.3053, ce_loss: 0.3053., kl_loss: 0.0000, accu: 0.8156, speed: 10.69 step/s\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.2696, ce_loss: 0.2696., kl_loss: 0.0000, accu: 0.8113, speed: 9.85 step/s\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.4015, ce_loss: 0.4015., kl_loss: 0.0000, accu: 0.8094, speed: 9.69 step/s\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.4170, ce_loss: 0.4170., kl_loss: 0.0000, accu: 0.8121, speed: 9.57 step/s\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.5086, ce_loss: 0.5086., kl_loss: 0.0000, accu: 0.8148, speed: 9.48 step/s\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.3321, ce_loss: 0.3321., kl_loss: 0.0000, accu: 0.8135, speed: 10.07 step/s\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.2575, ce_loss: 0.2575., kl_loss: 0.0000, accu: 0.8163, speed: 10.08 step/s\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.5305, ce_loss: 0.5305., kl_loss: 0.0000, accu: 0.8131, speed: 8.92 step/s\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.4481, ce_loss: 0.4481., kl_loss: 0.0000, accu: 0.8161, speed: 9.71 step/s\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.3517, ce_loss: 0.3517., kl_loss: 0.0000, accu: 0.8159, speed: 10.25 step/s\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.4341, ce_loss: 0.4341., kl_loss: 0.0000, accu: 0.8158, speed: 10.76 step/s\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.4970, ce_loss: 0.4970., kl_loss: 0.0000, accu: 0.8177, speed: 10.48 step/s\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.2409, ce_loss: 0.2409., kl_loss: 0.0000, accu: 0.8189, speed: 9.52 step/s\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.3609, ce_loss: 0.3609., kl_loss: 0.0000, accu: 0.8165, speed: 10.24 step/s\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.4755, ce_loss: 0.4755., kl_loss: 0.0000, accu: 0.8135, speed: 10.39 step/s\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.4339, ce_loss: 0.4339., kl_loss: 0.0000, accu: 0.8135, speed: 10.85 step/s\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.1610, ce_loss: 0.1610., kl_loss: 0.0000, accu: 0.8153, speed: 10.87 step/s\n",
      "dev_loss: 0.46685, accuracy: 0.79904, total_num:28802\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.3003, ce_loss: 0.3003., kl_loss: 0.0000, accu: 0.8063, speed: 0.29 step/s\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.5100, ce_loss: 0.5100., kl_loss: 0.0000, accu: 0.8125, speed: 10.04 step/s\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.4238, ce_loss: 0.4238., kl_loss: 0.0000, accu: 0.8073, speed: 9.29 step/s\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.4584, ce_loss: 0.4584., kl_loss: 0.0000, accu: 0.8039, speed: 9.50 step/s\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.4928, ce_loss: 0.4928., kl_loss: 0.0000, accu: 0.8100, speed: 9.81 step/s\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.5249, ce_loss: 0.5249., kl_loss: 0.0000, accu: 0.8089, speed: 9.18 step/s\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.3683, ce_loss: 0.3683., kl_loss: 0.0000, accu: 0.8134, speed: 9.48 step/s\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.4050, ce_loss: 0.4050., kl_loss: 0.0000, accu: 0.8148, speed: 10.26 step/s\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.3601, ce_loss: 0.3601., kl_loss: 0.0000, accu: 0.8181, speed: 9.06 step/s\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.3862, ce_loss: 0.3862., kl_loss: 0.0000, accu: 0.8181, speed: 9.04 step/s\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.2692, ce_loss: 0.2692., kl_loss: 0.0000, accu: 0.8173, speed: 9.26 step/s\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.3789, ce_loss: 0.3789., kl_loss: 0.0000, accu: 0.8148, speed: 10.13 step/s\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.3960, ce_loss: 0.3960., kl_loss: 0.0000, accu: 0.8142, speed: 10.17 step/s\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.2440, ce_loss: 0.2440., kl_loss: 0.0000, accu: 0.8141, speed: 9.27 step/s\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.2930, ce_loss: 0.2930., kl_loss: 0.0000, accu: 0.8125, speed: 9.56 step/s\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.5274, ce_loss: 0.5274., kl_loss: 0.0000, accu: 0.8117, speed: 9.69 step/s\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.2861, ce_loss: 0.2861., kl_loss: 0.0000, accu: 0.8121, speed: 9.76 step/s\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.5136, ce_loss: 0.5136., kl_loss: 0.0000, accu: 0.8116, speed: 10.16 step/s\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.3051, ce_loss: 0.3051., kl_loss: 0.0000, accu: 0.8109, speed: 9.25 step/s\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.4359, ce_loss: 0.4359., kl_loss: 0.0000, accu: 0.8087, speed: 10.61 step/s\n",
      "dev_loss: 0.45168, accuracy: 0.79932, total_num:28802\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.3014, ce_loss: 0.3014., kl_loss: 0.0000, accu: 0.8250, speed: 0.29 step/s\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.3822, ce_loss: 0.3822., kl_loss: 0.0000, accu: 0.7969, speed: 8.75 step/s\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.3486, ce_loss: 0.3486., kl_loss: 0.0000, accu: 0.8104, speed: 9.56 step/s\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.2309, ce_loss: 0.2309., kl_loss: 0.0000, accu: 0.8156, speed: 10.27 step/s\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.4536, ce_loss: 0.4536., kl_loss: 0.0000, accu: 0.8169, speed: 10.52 step/s\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.3533, ce_loss: 0.3533., kl_loss: 0.0000, accu: 0.8187, speed: 9.78 step/s\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.4743, ce_loss: 0.4743., kl_loss: 0.0000, accu: 0.8183, speed: 9.74 step/s\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.6310, ce_loss: 0.6310., kl_loss: 0.0000, accu: 0.8129, speed: 9.82 step/s\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.3286, ce_loss: 0.3286., kl_loss: 0.0000, accu: 0.8146, speed: 9.69 step/s\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.4149, ce_loss: 0.4149., kl_loss: 0.0000, accu: 0.8137, speed: 11.42 step/s\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.4488, ce_loss: 0.4488., kl_loss: 0.0000, accu: 0.8145, speed: 9.71 step/s\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.4560, ce_loss: 0.4560., kl_loss: 0.0000, accu: 0.8115, speed: 9.98 step/s\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.3099, ce_loss: 0.3099., kl_loss: 0.0000, accu: 0.8135, speed: 8.96 step/s\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.4703, ce_loss: 0.4703., kl_loss: 0.0000, accu: 0.8156, speed: 9.98 step/s\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.3478, ce_loss: 0.3478., kl_loss: 0.0000, accu: 0.8152, speed: 9.79 step/s\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.2881, ce_loss: 0.2881., kl_loss: 0.0000, accu: 0.8162, speed: 9.85 step/s\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.4028, ce_loss: 0.4028., kl_loss: 0.0000, accu: 0.8171, speed: 9.26 step/s\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.4997, ce_loss: 0.4997., kl_loss: 0.0000, accu: 0.8168, speed: 10.30 step/s\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.2682, ce_loss: 0.2682., kl_loss: 0.0000, accu: 0.8192, speed: 10.49 step/s\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.4590, ce_loss: 0.4590., kl_loss: 0.0000, accu: 0.8203, speed: 10.32 step/s\n",
      "dev_loss: 0.48376, accuracy: 0.80418, total_num:28802\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.5222, ce_loss: 0.5222., kl_loss: 0.0000, accu: 0.8406, speed: 0.29 step/s\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.4215, ce_loss: 0.4215., kl_loss: 0.0000, accu: 0.8031, speed: 11.44 step/s\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.2679, ce_loss: 0.2679., kl_loss: 0.0000, accu: 0.7958, speed: 10.72 step/s\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.6016, ce_loss: 0.6016., kl_loss: 0.0000, accu: 0.8039, speed: 10.23 step/s\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.2663, ce_loss: 0.2663., kl_loss: 0.0000, accu: 0.8137, speed: 10.34 step/s\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.2803, ce_loss: 0.2803., kl_loss: 0.0000, accu: 0.8146, speed: 10.10 step/s\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.2880, ce_loss: 0.2880., kl_loss: 0.0000, accu: 0.8138, speed: 9.44 step/s\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.3339, ce_loss: 0.3339., kl_loss: 0.0000, accu: 0.8152, speed: 9.43 step/s\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.2948, ce_loss: 0.2948., kl_loss: 0.0000, accu: 0.8187, speed: 10.43 step/s\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.4863, ce_loss: 0.4863., kl_loss: 0.0000, accu: 0.8175, speed: 10.07 step/s\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.3895, ce_loss: 0.3895., kl_loss: 0.0000, accu: 0.8159, speed: 11.14 step/s\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.5149, ce_loss: 0.5149., kl_loss: 0.0000, accu: 0.8182, speed: 9.34 step/s\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.2004, ce_loss: 0.2004., kl_loss: 0.0000, accu: 0.8202, speed: 9.34 step/s\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.5502, ce_loss: 0.5502., kl_loss: 0.0000, accu: 0.8210, speed: 9.39 step/s\n",
      "global step 1550, epoch: 1, batch: 1550, loss: 0.4881, ce_loss: 0.4881., kl_loss: 0.0000, accu: 0.8200, speed: 10.02 step/s\n",
      "global step 1560, epoch: 1, batch: 1560, loss: 0.4176, ce_loss: 0.4176., kl_loss: 0.0000, accu: 0.8195, speed: 10.02 step/s\n",
      "global step 1570, epoch: 1, batch: 1570, loss: 0.4090, ce_loss: 0.4090., kl_loss: 0.0000, accu: 0.8200, speed: 10.57 step/s\n",
      "global step 1580, epoch: 1, batch: 1580, loss: 0.2286, ce_loss: 0.2286., kl_loss: 0.0000, accu: 0.8219, speed: 10.71 step/s\n",
      "global step 1590, epoch: 1, batch: 1590, loss: 0.2268, ce_loss: 0.2268., kl_loss: 0.0000, accu: 0.8217, speed: 9.06 step/s\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.3945, ce_loss: 0.3945., kl_loss: 0.0000, accu: 0.8230, speed: 10.10 step/s\n",
      "dev_loss: 0.42537, accuracy: 0.80835, total_num:28802\n",
      "global step 1610, epoch: 1, batch: 1610, loss: 0.4064, ce_loss: 0.4064., kl_loss: 0.0000, accu: 0.8406, speed: 0.29 step/s\n",
      "global step 1620, epoch: 1, batch: 1620, loss: 0.5278, ce_loss: 0.5278., kl_loss: 0.0000, accu: 0.8406, speed: 9.48 step/s\n",
      "global step 1630, epoch: 1, batch: 1630, loss: 0.3832, ce_loss: 0.3832., kl_loss: 0.0000, accu: 0.8406, speed: 10.17 step/s\n",
      "global step 1640, epoch: 1, batch: 1640, loss: 0.4317, ce_loss: 0.4317., kl_loss: 0.0000, accu: 0.8375, speed: 10.13 step/s\n",
      "global step 1650, epoch: 1, batch: 1650, loss: 0.3482, ce_loss: 0.3482., kl_loss: 0.0000, accu: 0.8400, speed: 10.38 step/s\n",
      "global step 1660, epoch: 1, batch: 1660, loss: 0.5504, ce_loss: 0.5504., kl_loss: 0.0000, accu: 0.8385, speed: 9.82 step/s\n",
      "global step 1670, epoch: 1, batch: 1670, loss: 0.4337, ce_loss: 0.4337., kl_loss: 0.0000, accu: 0.8433, speed: 9.51 step/s\n",
      "global step 1680, epoch: 1, batch: 1680, loss: 0.4354, ce_loss: 0.4354., kl_loss: 0.0000, accu: 0.8402, speed: 9.44 step/s\n",
      "global step 1690, epoch: 1, batch: 1690, loss: 0.4566, ce_loss: 0.4566., kl_loss: 0.0000, accu: 0.8417, speed: 11.29 step/s\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.4430, ce_loss: 0.4430., kl_loss: 0.0000, accu: 0.8406, speed: 9.84 step/s\n",
      "global step 1710, epoch: 1, batch: 1710, loss: 0.7899, ce_loss: 0.7899., kl_loss: 0.0000, accu: 0.8375, speed: 8.68 step/s\n",
      "global step 1720, epoch: 1, batch: 1720, loss: 0.3385, ce_loss: 0.3385., kl_loss: 0.0000, accu: 0.8388, speed: 9.74 step/s\n",
      "global step 1730, epoch: 1, batch: 1730, loss: 0.2568, ce_loss: 0.2568., kl_loss: 0.0000, accu: 0.8358, speed: 10.74 step/s\n",
      "global step 1740, epoch: 1, batch: 1740, loss: 0.3859, ce_loss: 0.3859., kl_loss: 0.0000, accu: 0.8355, speed: 9.25 step/s\n",
      "global step 1750, epoch: 1, batch: 1750, loss: 0.2457, ce_loss: 0.2457., kl_loss: 0.0000, accu: 0.8369, speed: 10.28 step/s\n",
      "global step 1760, epoch: 1, batch: 1760, loss: 0.4325, ce_loss: 0.4325., kl_loss: 0.0000, accu: 0.8363, speed: 9.46 step/s\n",
      "global step 1770, epoch: 1, batch: 1770, loss: 0.4933, ce_loss: 0.4933., kl_loss: 0.0000, accu: 0.8344, speed: 10.35 step/s\n",
      "global step 1780, epoch: 1, batch: 1780, loss: 0.3413, ce_loss: 0.3413., kl_loss: 0.0000, accu: 0.8349, speed: 8.69 step/s\n",
      "global step 1790, epoch: 1, batch: 1790, loss: 0.2885, ce_loss: 0.2885., kl_loss: 0.0000, accu: 0.8332, speed: 9.04 step/s\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.2213, ce_loss: 0.2213., kl_loss: 0.0000, accu: 0.8320, speed: 11.50 step/s\n",
      "dev_loss: 0.44297, accuracy: 0.80845, total_num:28802\n",
      "global step 1810, epoch: 1, batch: 1810, loss: 0.4833, ce_loss: 0.4833., kl_loss: 0.0000, accu: 0.8219, speed: 0.29 step/s\n",
      "global step 1820, epoch: 1, batch: 1820, loss: 0.4707, ce_loss: 0.4707., kl_loss: 0.0000, accu: 0.8063, speed: 9.48 step/s\n",
      "global step 1830, epoch: 1, batch: 1830, loss: 0.2741, ce_loss: 0.2741., kl_loss: 0.0000, accu: 0.8094, speed: 10.28 step/s\n",
      "global step 1840, epoch: 1, batch: 1840, loss: 0.2589, ce_loss: 0.2589., kl_loss: 0.0000, accu: 0.8211, speed: 9.90 step/s\n",
      "global step 1850, epoch: 1, batch: 1850, loss: 0.3909, ce_loss: 0.3909., kl_loss: 0.0000, accu: 0.8213, speed: 10.38 step/s\n",
      "global step 1860, epoch: 1, batch: 1860, loss: 0.2645, ce_loss: 0.2645., kl_loss: 0.0000, accu: 0.8260, speed: 10.13 step/s\n",
      "global step 1870, epoch: 1, batch: 1870, loss: 0.3965, ce_loss: 0.3965., kl_loss: 0.0000, accu: 0.8254, speed: 10.03 step/s\n",
      "global step 1880, epoch: 1, batch: 1880, loss: 0.1371, ce_loss: 0.1371., kl_loss: 0.0000, accu: 0.8273, speed: 10.45 step/s\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "best_accuracy = 0.0\n",
    "\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, 3 + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits1, kl_loss = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        correct = metric.compute(logits1, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        ce_loss = criterion(logits1, labels)\n",
    "        if kl_loss > 0:\n",
    "            loss = ce_loss + kl_loss * args.rdrop_coef\n",
    "        else:\n",
    "            loss = ce_loss\n",
    "            \n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.4f, ce_loss: %.4f., kl_loss: %.4f, accu: %.4f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, ce_loss, kl_loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 200 == 0:\n",
    "            accuracy = evaluate(model, criterion, metric, dev_data_loader)\n",
    "            if accuracy > best_accuracy:\n",
    "                save_dir = os.path.join(\"./checkpoint\", \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                tokenizer.save_pretrained(save_dir)\n",
    "                best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.5 模型预测\n",
    "\n",
    "接下来我们使用已经训练好的语义匹配模型对一些预测数据进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! wget https://paddlenlp.bj.bcebos.com/models/text_matching/question_matching_rdrop0p0_baseline_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! tar -xvf question_matching_rdrop0p0_baseline_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -3 \"data/data104941/test_A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$ unset CUDA_VISIBLE_DEVICES\n",
    "!python -u \\\n",
    "    work/predict.py \\\n",
    "    --device gpu \\\n",
    "    --params_path \"./ernie_gram_rdrop0p0/model_state.pdparams\" \\\n",
    "    --batch_size 128 \\\n",
    "    --input_file \"data/data104941/test_A\" \\\n",
    "    --result_file \"ccf_qianyan_qm_result_A.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 提交预测结果[2021 CCF BDCI 千言-问题匹配鲁棒性评测比赛👈](https://aistudio.baidu.com/aistudio/competition/detail/116/0/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 更多优化方案\n",
    "\n",
    "- 对抗训练\n",
    "\n",
    "- 数据增强\n",
    "\n",
    "- 大模型\n",
    "\n",
    "- 模型集成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
