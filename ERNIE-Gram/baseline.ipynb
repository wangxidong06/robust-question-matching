{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2021 CCF BDCI åƒè¨€-é—®é¢˜åŒ¹é…é²æ£’æ€§è¯„æµ‹Baseline\n",
    "\n",
    "\n",
    "**[2021 CCF BDCI åƒè¨€-é—®é¢˜åŒ¹é…é²æ£’æ€§è¯„æµ‹æ¯”èµ›ğŸ‘ˆ](https://aistudio.baidu.com/aistudio/competition/detail/116/0/introduction)**\n",
    "\n",
    "**[Baseline GithubğŸ‘ˆ](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/text_matching/question_matching)**\n",
    "\n",
    "æœ¬æ¡ˆä¾‹ä»‹ç» NLP æœ€åŸºæœ¬çš„ä»»åŠ¡ç±»å‹ä¹‹ä¸€ â€”â€” æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼Œå¹¶ä¸”åŸºäº PaddleNLP ä½¿ç”¨ç™¾åº¦å¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram æ­å»ºæ•ˆæœä¼˜å¼‚çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹ï¼Œæ¥åˆ¤æ–­ 2 æ®µæ–‡æœ¬è¯­ä¹‰æ˜¯å¦ç›¸åŒã€‚\n",
    "\n",
    "## 1. èƒŒæ™¯ä»‹ç»\n",
    "æ–‡æœ¬è¯­ä¹‰åŒ¹é…ä»»åŠ¡ï¼Œç®€å•æ¥è¯´å°±æ˜¯ç»™å®šä¸¤æ®µæ–‡æœ¬ï¼Œè®©æ¨¡å‹æ¥åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬æ˜¯ä¸æ˜¯è¯­ä¹‰ç›¸ä¼¼ã€‚\n",
    "\n",
    "ä»¥æƒå¨çš„è¯­ä¹‰åŒ¹é…æ•°æ®é›† [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) ä¸ºä¾‹ï¼Œ[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ•°æ®é›†æ˜¯åŸºäºç™¾åº¦çŸ¥é“ç›¸ä¼¼é—®é¢˜æ¨èæ„é€ çš„é€šé—®å¥è¯­ä¹‰åŒ¹é…æ•°æ®é›†ã€‚è®­ç»ƒé›†ä¸­çš„æ¯ä¸¤æ®µæ–‡æœ¬éƒ½ä¼šè¢«æ ‡è®°ä¸º 1ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰ æˆ–è€… 0ï¼ˆè¯­ä¹‰ä¸ç›¸ä¼¼ï¼‰ã€‚æ›´å¤šæ•°æ®é›†å¯è®¿é—®[åƒè¨€](https://www.luge.ai/)è·å–å“¦ã€‚\n",
    "\n",
    "ä¾‹å¦‚ç™¾åº¦çŸ¥é“åœºæ™¯ä¸‹ï¼Œç”¨æˆ·æœç´¢ä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ä¼šè®¡ç®—è¿™ä¸ªé—®é¢˜ä¸å€™é€‰é—®é¢˜æ˜¯å¦è¯­ä¹‰ç›¸ä¼¼ï¼Œè¯­ä¹‰åŒ¹é…æ¨¡å‹ä¼šæ‰¾å‡ºä¸é—®é¢˜è¯­ä¹‰ç›¸ä¼¼çš„å€™é€‰é—®é¢˜è¿”å›ç»™ç”¨æˆ·ï¼ŒåŠ å¿«ç”¨æˆ·æé—®-è·å–ç­”æ¡ˆçš„æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œå½“æŸç”¨æˆ·åœ¨æœç´¢å¼•æ“ä¸­æœç´¢ â€œæ·±åº¦å­¦ä¹ çš„æ•™ææœ‰å“ªäº›ï¼Ÿâ€ï¼Œæ¨¡å‹å°±è‡ªåŠ¨æ‰¾åˆ°äº†ä¸€äº›è¯­ä¹‰ç›¸ä¼¼çš„é—®é¢˜å±•ç°ç»™ç”¨æˆ·:\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/ecc1244685ec4476b869ce8a32d421c0ad530666e98d487da21fa4f61670544f\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.å¿«é€Ÿå®è·µ\n",
    "\n",
    "ä»‹ç»å¦‚ä½•å‡†å¤‡æ•°æ®ï¼ŒåŸºäº ERNIE-Gram æ¨¡å‹æ­å»ºåŒ¹é…ç½‘ç»œï¼Œç„¶åå¿«é€Ÿè¿›è¡Œè¯­ä¹‰åŒ¹é…æ¨¡å‹çš„è®­ç»ƒã€è¯„ä¼°å’Œé¢„æµ‹ã€‚\n",
    "\n",
    "### 2.1 æ•°æ®åŠ è½½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already up-to-date: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: paddlefsl==1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl==1.0.0->paddlenlp) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: requests~=2.24.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl==1.0.0->paddlenlp) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm~=4.27.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl==1.0.0->paddlenlp) (4.27.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow==8.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl==1.0.0->paddlenlp) (8.2.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "# æ­£å¼å¼€å§‹å®éªŒä¹‹å‰é¦–å…ˆé€šè¿‡å¦‚ä¸‹å‘½ä»¤å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ paddlenlp\n",
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data\r\n",
      "â”œâ”€â”€ data104940\r\n",
      "â”‚Â Â  â””â”€â”€ train.zip\r\n",
      "â”œâ”€â”€ data104941\r\n",
      "â”‚Â Â  â””â”€â”€ test_A\r\n",
      "â”œâ”€â”€ __MACOSX\r\n",
      "â”‚Â Â  â””â”€â”€ train\r\n",
      "â”‚Â Â      â”œâ”€â”€ BQ\r\n",
      "â”‚Â Â      â”œâ”€â”€ LCQMC\r\n",
      "â”‚Â Â      â””â”€â”€ OPPO\r\n",
      "â””â”€â”€ train\r\n",
      "    â”œâ”€â”€ BQ\r\n",
      "    â”‚Â Â  â”œâ”€â”€ dev\r\n",
      "    â”‚Â Â  â”œâ”€â”€ test\r\n",
      "    â”‚Â Â  â””â”€â”€ train\r\n",
      "    â”œâ”€â”€ LCQMC\r\n",
      "    â”‚Â Â  â”œâ”€â”€ dev\r\n",
      "    â”‚Â Â  â”œâ”€â”€ test\r\n",
      "    â”‚Â Â  â””â”€â”€ train\r\n",
      "    â””â”€â”€ OPPO\r\n",
      "        â”œâ”€â”€ dev\r\n",
      "        â””â”€â”€ train\r\n",
      "\r\n",
      "11 directories, 10 files\r\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥æ•°æ®é›†æ‰€åœ¨è·¯å¾„\n",
    "!tree -L 3 /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/data104940/train.zip\n",
      "  inflating: data/train/.DS_Store    \n",
      "  inflating: data/__MACOSX/train/._.DS_Store  \n",
      "  inflating: data/train/BQ/test      \n",
      "  inflating: data/__MACOSX/train/BQ/._test  \n",
      "  inflating: data/train/BQ/train     \n",
      "  inflating: data/__MACOSX/train/BQ/._train  \n",
      "  inflating: data/train/BQ/dev       \n",
      "  inflating: data/__MACOSX/train/BQ/._dev  \n",
      "  inflating: data/__MACOSX/train/._BQ  \n",
      "  inflating: data/train/LCQMC/test   \n",
      "  inflating: data/__MACOSX/train/LCQMC/._test  \n",
      "  inflating: data/train/LCQMC/train  \n",
      "  inflating: data/__MACOSX/train/LCQMC/._train  \n",
      "  inflating: data/train/LCQMC/dev    \n",
      "  inflating: data/__MACOSX/train/LCQMC/._dev  \n",
      "  inflating: data/__MACOSX/train/._LCQMC  \n",
      "  inflating: data/train/OPPO/train   \n",
      "  inflating: data/__MACOSX/train/OPPO/._train  \n",
      "  inflating: data/train/OPPO/dev     \n",
      "  inflating: data/__MACOSX/train/OPPO/._dev  \n",
      "  inflating: data/__MACOSX/train/._OPPO  \n",
      "  inflating: data/__MACOSX/._train   \n"
     ]
    }
   ],
   "source": [
    "!unzip -o data/data104940/train.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# å°†LCQMCã€BQã€OPPOä¸‰ä¸ªæ•°æ®é›†çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆå¹¶\n",
    "!cat ./data/train/LCQMC/train ./data/train/BQ/train ./data/train/OPPO/train > train.txt\n",
    "!cat ./data/train/LCQMC/dev ./data/train/BQ/dev ./data/train/OPPO/dev > dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "from work.data import create_dataloader, read_text_pair, convert_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset(read_text_pair, data_path=\"train.txt\", is_test=False, lazy=False)\n",
    "\n",
    "dev_ds = load_dataset(read_text_pair, data_path=\"dev.txt\", is_test=False, lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query1': 'å–œæ¬¢æ‰“ç¯®çƒçš„ç”·ç”Ÿå–œæ¬¢ä»€ä¹ˆæ ·çš„å¥³ç”Ÿ', 'query2': 'çˆ±æ‰“ç¯®çƒçš„ç”·ç”Ÿå–œæ¬¢ä»€ä¹ˆæ ·çš„å¥³ç”Ÿ', 'label': '1'}\n",
      "{'query1': 'æˆ‘æ‰‹æœºä¸¢äº†ï¼Œæˆ‘æƒ³æ¢ä¸ªæ‰‹æœº', 'query2': 'æˆ‘æƒ³ä¹°ä¸ªæ–°æ‰‹æœºï¼Œæ±‚æ¨è', 'label': '1'}\n",
      "{'query1': 'å¤§å®¶è§‰å¾—å¥¹å¥½çœ‹å—', 'query2': 'å¤§å®¶è§‰å¾—è·‘ç”·å¥½çœ‹å—ï¼Ÿ', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "# è¾“å‡ºè®­ç»ƒé›†çš„å‰ 3 æ¡æ ·æœ¬\n",
    "for idx, example in enumerate(train_ds):\n",
    "    if idx <= 2:\n",
    "        print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "é€šè¿‡ PaddleNLP åŠ è½½è¿›æ¥çš„æ•°æ®é›†æ˜¯åŸå§‹çš„æ˜æ–‡æ•°æ®é›†ï¼Œè¿™éƒ¨åˆ†æˆ‘ä»¬æ¥å®ç°ç»„ batchã€tokenize ç­‰é¢„å¤„ç†é€»è¾‘ï¼Œå°†åŸå§‹æ˜æ–‡æ•°æ®è½¬æ¢æˆç½‘ç»œè®­ç»ƒçš„è¾“å…¥æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-24 23:48:10,370] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# å› ä¸ºæ˜¯åŸºäºé¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram æ¥è¿›è¡Œï¼Œæ‰€ä»¥éœ€è¦é¦–å…ˆåŠ è½½ ERNIE-Gram çš„ tokenizerï¼Œ\n",
    "# åç»­æ ·æœ¬è½¬æ¢å‡½æ•°åŸºäº tokenizer å¯¹æ–‡æœ¬è¿›è¡Œåˆ‡åˆ†\n",
    "tokenizer = ppnlp.transformers.ErnieGramTokenizer.from_pretrained('ernie-gram-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### å¯¹è®­ç»ƒé›†çš„ç¬¬ 1 æ¡æ•°æ®è¿›è¡Œè½¬æ¢\n",
    "input_ids, token_type_ids, label = convert_example(train_ds[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 692, 811, 445, 2001, 497, 5, 654, 21, 692, 811, 614, 356, 314, 5, 291, 21, 2, 329, 445, 2001, 497, 5, 654, 21, 692, 811, 614, 356, 314, 5, 291, 21, 2]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ä¸ºäº†åç»­æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨pythonåå‡½æ•°ï¼ˆpartialï¼‰ç»™ convert_example èµ‹äºˆä¸€äº›é»˜è®¤å‚æ•°\n",
    "# è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ ·æœ¬è½¬æ¢å‡½æ•°\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, max_seq_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ä¼šè¿”å› input_ids, token_type_ids, labels 3 ä¸ªå­—æ®µ\n",
    "# å› æ­¤é’ˆå¯¹è¿™ 3 ä¸ªå­—æ®µéœ€è¦åˆ†åˆ«å®šä¹‰ 3 ä¸ªç»„ batch æ“ä½œ\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # text_pair_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # text_pair_segment\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### å®šä¹‰ Dataloader\n",
    "ä¸‹é¢æˆ‘ä»¬åŸºäºç»„ batchify_fn å‡½æ•°å’Œæ ·æœ¬è½¬æ¢å‡½æ•° trans_func æ¥æ„é€ è®­ç»ƒé›†çš„ DataLoader, æ”¯æŒå¤šå¡è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=32,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "dev_data_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        mode='dev',\n",
    "        batch_size=128,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 æ¨¡å‹æ­å»º\n",
    "\n",
    "è‡ªä» 2018 å¹´ 10 æœˆä»¥æ¥ï¼ŒNLP ä¸ªé¢†åŸŸçš„ä»»åŠ¡éƒ½é€šè¿‡ Pretrain + Finetune çš„æ¨¡å¼ç›¸æ¯”ä¼ ç»Ÿ DNN æ–¹æ³•åœ¨æ•ˆæœä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œæœ¬èŠ‚æˆ‘ä»¬ä»¥ç™¾åº¦å¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram ä¸ºåŸºç¡€æ¨¡å‹ï¼Œåœ¨æ­¤ä¹‹ä¸Šæ„å»º Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œã€‚\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬æ¥å®šä¹‰ç½‘ç»œç»“æ„:\n",
    "\n",
    "- Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œ\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/e5415a9928064f299e94f91fcc0e4ce6edc98cf718c44c66a5084e672ae6ef78\" width=\"600\" />\n",
    "\n",
    "- ERNIE-Gram\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/7a9c14983bbe4236ad90bcc45258acfa32704668549541909947803be338e2a3\" width=\"600\" />\n",
    "\n",
    "- Rdrop\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d09ce0ffecfe4c849366c5a3ce04e8c2b266a92cba294df38a3c4700d313be18\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-24 23:48:33,426] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/ernie_gram_zh.pdparams\n"
     ]
    }
   ],
   "source": [
    "# æˆ‘ä»¬åŸºäº ERNIE-Gram æ¨¡å‹ç»“æ„æ­å»º Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œ\n",
    "# æ‰€ä»¥æ­¤å¤„å…ˆå®šä¹‰ ERNIE-Gram çš„ pretrained_model\n",
    "pretrained_model = ppnlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
    "\n",
    "class QuestionMatching(nn.Layer):\n",
    "    def __init__(self, pretrained_model, dropout=None, rdrop_coef=0.0):\n",
    "        super().__init__()\n",
    "        self.ptm = pretrained_model\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "\n",
    "        # num_labels = 2 (similar or dissimilar)\n",
    "        self.classifier = nn.Linear(self.ptm.config[\"hidden_size\"], 2)\n",
    "        self.rdrop_coef = rdrop_coef\n",
    "        self.rdrop_loss = ppnlp.losses.RDropLoss()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                attention_mask=None,\n",
    "                do_evaluate=False):\n",
    "\n",
    "        _, cls_embedding1 = self.ptm(input_ids, token_type_ids, position_ids, attention_mask)\n",
    "        cls_embedding1 = self.dropout(cls_embedding1)\n",
    "        logits1 = self.classifier(cls_embedding1)\n",
    "        \n",
    "        # For more information about R-drop please refer to this paper: https://arxiv.org/abs/2106.14448\n",
    "        # Original implementation please refer to this code: https://github.com/dropreg/R-Drop\n",
    "        if self.rdrop_coef > 0 and not do_evaluate:\n",
    "            _, cls_embedding2 = self.ptm(input_ids, token_type_ids, position_ids, attention_mask)\n",
    "            cls_embedding2 = self.dropout(cls_embedding2)\n",
    "            logits2 = self.classifier(cls_embedding2)\n",
    "            kl_loss = self.rdrop_loss(logits1, logits2)\n",
    "        else:\n",
    "            kl_loss = 0.0\n",
    "\n",
    "        return logits1, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = QuestionMatching(pretrained_model, rdrop_coef=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 æ¨¡å‹è®­ç»ƒ & è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "lr_scheduler = LinearDecayWithWarmup(5e-5, num_training_steps, 0.0)\n",
    "\n",
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    \n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=0.0,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    total_num = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        total_num += len(labels)\n",
    "        logits, _ = model(input_ids=input_ids, token_type_ids=token_type_ids, do_evaluate=True)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "\n",
    "    print(\"dev_loss: {:.5}, accuracy: {:.5}, total_num:{}\".format(np.mean(losses), accu, total_num))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.6175, ce_loss: 0.6175., kl_loss: 0.0000, accu: 0.5813, speed: 7.59 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.5776, ce_loss: 0.5776., kl_loss: 0.0000, accu: 0.6000, speed: 9.83 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.5113, ce_loss: 0.5113., kl_loss: 0.0000, accu: 0.6260, speed: 9.80 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.5664, ce_loss: 0.5664., kl_loss: 0.0000, accu: 0.6461, speed: 9.78 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.6439, ce_loss: 0.6439., kl_loss: 0.0000, accu: 0.6600, speed: 11.01 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.6244, ce_loss: 0.6244., kl_loss: 0.0000, accu: 0.6687, speed: 9.87 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.3942, ce_loss: 0.3942., kl_loss: 0.0000, accu: 0.6857, speed: 9.60 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.6121, ce_loss: 0.6121., kl_loss: 0.0000, accu: 0.6918, speed: 10.21 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.3481, ce_loss: 0.3481., kl_loss: 0.0000, accu: 0.6990, speed: 10.77 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.5572, ce_loss: 0.5572., kl_loss: 0.0000, accu: 0.7016, speed: 9.63 step/s\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.5894, ce_loss: 0.5894., kl_loss: 0.0000, accu: 0.7051, speed: 10.13 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.5046, ce_loss: 0.5046., kl_loss: 0.0000, accu: 0.7107, speed: 10.26 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.5550, ce_loss: 0.5550., kl_loss: 0.0000, accu: 0.7163, speed: 9.12 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.3854, ce_loss: 0.3854., kl_loss: 0.0000, accu: 0.7172, speed: 9.84 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.6192, ce_loss: 0.6192., kl_loss: 0.0000, accu: 0.7210, speed: 9.38 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.4645, ce_loss: 0.4645., kl_loss: 0.0000, accu: 0.7252, speed: 9.53 step/s\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.3939, ce_loss: 0.3939., kl_loss: 0.0000, accu: 0.7278, speed: 10.46 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.5798, ce_loss: 0.5798., kl_loss: 0.0000, accu: 0.7321, speed: 11.09 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.5599, ce_loss: 0.5599., kl_loss: 0.0000, accu: 0.7329, speed: 10.26 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.3266, ce_loss: 0.3266., kl_loss: 0.0000, accu: 0.7362, speed: 10.39 step/s\n",
      "dev_loss: 0.49613, accuracy: 0.77009, total_num:28802\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.4479, ce_loss: 0.4479., kl_loss: 0.0000, accu: 0.7937, speed: 0.29 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.5980, ce_loss: 0.5980., kl_loss: 0.0000, accu: 0.8094, speed: 10.63 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.4304, ce_loss: 0.4304., kl_loss: 0.0000, accu: 0.8073, speed: 10.11 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.2967, ce_loss: 0.2967., kl_loss: 0.0000, accu: 0.8031, speed: 9.93 step/s\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.4559, ce_loss: 0.4559., kl_loss: 0.0000, accu: 0.7975, speed: 9.87 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.5846, ce_loss: 0.5846., kl_loss: 0.0000, accu: 0.7927, speed: 9.39 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.5715, ce_loss: 0.5715., kl_loss: 0.0000, accu: 0.7920, speed: 9.64 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.4105, ce_loss: 0.4105., kl_loss: 0.0000, accu: 0.7922, speed: 10.14 step/s\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.5934, ce_loss: 0.5934., kl_loss: 0.0000, accu: 0.7889, speed: 9.38 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.5936, ce_loss: 0.5936., kl_loss: 0.0000, accu: 0.7887, speed: 9.93 step/s\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.2517, ce_loss: 0.2517., kl_loss: 0.0000, accu: 0.7889, speed: 9.35 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.5683, ce_loss: 0.5683., kl_loss: 0.0000, accu: 0.7878, speed: 10.49 step/s\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.3709, ce_loss: 0.3709., kl_loss: 0.0000, accu: 0.7894, speed: 10.41 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.5695, ce_loss: 0.5695., kl_loss: 0.0000, accu: 0.7931, speed: 9.72 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.5525, ce_loss: 0.5525., kl_loss: 0.0000, accu: 0.7931, speed: 9.82 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.4392, ce_loss: 0.4392., kl_loss: 0.0000, accu: 0.7900, speed: 9.61 step/s\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.3646, ce_loss: 0.3646., kl_loss: 0.0000, accu: 0.7893, speed: 10.28 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.4094, ce_loss: 0.4094., kl_loss: 0.0000, accu: 0.7903, speed: 11.31 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.5652, ce_loss: 0.5652., kl_loss: 0.0000, accu: 0.7885, speed: 11.08 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.4017, ce_loss: 0.4017., kl_loss: 0.0000, accu: 0.7889, speed: 9.25 step/s\n",
      "dev_loss: 0.54604, accuracy: 0.75745, total_num:28802\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.6149, ce_loss: 0.6149., kl_loss: 0.0000, accu: 0.7844, speed: 0.30 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.2831, ce_loss: 0.2831., kl_loss: 0.0000, accu: 0.7891, speed: 9.89 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.5227, ce_loss: 0.5227., kl_loss: 0.0000, accu: 0.7833, speed: 9.92 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.3020, ce_loss: 0.3020., kl_loss: 0.0000, accu: 0.7875, speed: 9.94 step/s\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.4327, ce_loss: 0.4327., kl_loss: 0.0000, accu: 0.8000, speed: 9.89 step/s\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.5397, ce_loss: 0.5397., kl_loss: 0.0000, accu: 0.8016, speed: 10.31 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.6421, ce_loss: 0.6421., kl_loss: 0.0000, accu: 0.8013, speed: 10.12 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.3306, ce_loss: 0.3306., kl_loss: 0.0000, accu: 0.8059, speed: 9.93 step/s\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.5526, ce_loss: 0.5526., kl_loss: 0.0000, accu: 0.8063, speed: 9.56 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.7031, ce_loss: 0.7031., kl_loss: 0.0000, accu: 0.8019, speed: 10.19 step/s\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.3049, ce_loss: 0.3049., kl_loss: 0.0000, accu: 0.8017, speed: 10.53 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.4084, ce_loss: 0.4084., kl_loss: 0.0000, accu: 0.8031, speed: 10.28 step/s\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.3958, ce_loss: 0.3958., kl_loss: 0.0000, accu: 0.8012, speed: 10.55 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.7291, ce_loss: 0.7291., kl_loss: 0.0000, accu: 0.7991, speed: 9.46 step/s\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.5164, ce_loss: 0.5164., kl_loss: 0.0000, accu: 0.7987, speed: 10.02 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.5365, ce_loss: 0.5365., kl_loss: 0.0000, accu: 0.7975, speed: 9.71 step/s\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.3956, ce_loss: 0.3956., kl_loss: 0.0000, accu: 0.7998, speed: 8.93 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.2960, ce_loss: 0.2960., kl_loss: 0.0000, accu: 0.8014, speed: 10.06 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.3228, ce_loss: 0.3228., kl_loss: 0.0000, accu: 0.8007, speed: 10.52 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.4265, ce_loss: 0.4265., kl_loss: 0.0000, accu: 0.7983, speed: 10.36 step/s\n",
      "dev_loss: 0.45913, accuracy: 0.7896, total_num:28802\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.2518, ce_loss: 0.2518., kl_loss: 0.0000, accu: 0.8031, speed: 0.29 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.2792, ce_loss: 0.2792., kl_loss: 0.0000, accu: 0.8078, speed: 10.60 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.5155, ce_loss: 0.5155., kl_loss: 0.0000, accu: 0.8146, speed: 10.53 step/s\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.3014, ce_loss: 0.3014., kl_loss: 0.0000, accu: 0.8141, speed: 10.16 step/s\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.8349, ce_loss: 0.8349., kl_loss: 0.0000, accu: 0.8144, speed: 10.29 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.4453, ce_loss: 0.4453., kl_loss: 0.0000, accu: 0.8104, speed: 9.97 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.4124, ce_loss: 0.4124., kl_loss: 0.0000, accu: 0.8094, speed: 9.95 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.5378, ce_loss: 0.5378., kl_loss: 0.0000, accu: 0.8121, speed: 9.65 step/s\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.3940, ce_loss: 0.3940., kl_loss: 0.0000, accu: 0.8069, speed: 10.70 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.3961, ce_loss: 0.3961., kl_loss: 0.0000, accu: 0.8094, speed: 11.40 step/s\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.2661, ce_loss: 0.2661., kl_loss: 0.0000, accu: 0.8099, speed: 10.93 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.5099, ce_loss: 0.5099., kl_loss: 0.0000, accu: 0.8094, speed: 9.72 step/s\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.5040, ce_loss: 0.5040., kl_loss: 0.0000, accu: 0.8084, speed: 10.42 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.3128, ce_loss: 0.3128., kl_loss: 0.0000, accu: 0.8105, speed: 9.70 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.2652, ce_loss: 0.2652., kl_loss: 0.0000, accu: 0.8079, speed: 9.58 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.5191, ce_loss: 0.5191., kl_loss: 0.0000, accu: 0.8102, speed: 9.61 step/s\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.4106, ce_loss: 0.4106., kl_loss: 0.0000, accu: 0.8118, speed: 9.20 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.6223, ce_loss: 0.6223., kl_loss: 0.0000, accu: 0.8116, speed: 9.52 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.4708, ce_loss: 0.4708., kl_loss: 0.0000, accu: 0.8141, speed: 9.86 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.4898, ce_loss: 0.4898., kl_loss: 0.0000, accu: 0.8141, speed: 9.86 step/s\n",
      "dev_loss: 0.46854, accuracy: 0.78321, total_num:28802\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.4737, ce_loss: 0.4737., kl_loss: 0.0000, accu: 0.8094, speed: 0.30 step/s\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.4140, ce_loss: 0.4140., kl_loss: 0.0000, accu: 0.8078, speed: 8.97 step/s\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.4691, ce_loss: 0.4691., kl_loss: 0.0000, accu: 0.8052, speed: 10.00 step/s\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.3053, ce_loss: 0.3053., kl_loss: 0.0000, accu: 0.8156, speed: 10.69 step/s\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.2696, ce_loss: 0.2696., kl_loss: 0.0000, accu: 0.8113, speed: 9.85 step/s\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.4015, ce_loss: 0.4015., kl_loss: 0.0000, accu: 0.8094, speed: 9.69 step/s\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.4170, ce_loss: 0.4170., kl_loss: 0.0000, accu: 0.8121, speed: 9.57 step/s\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.5086, ce_loss: 0.5086., kl_loss: 0.0000, accu: 0.8148, speed: 9.48 step/s\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.3321, ce_loss: 0.3321., kl_loss: 0.0000, accu: 0.8135, speed: 10.07 step/s\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.2575, ce_loss: 0.2575., kl_loss: 0.0000, accu: 0.8163, speed: 10.08 step/s\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.5305, ce_loss: 0.5305., kl_loss: 0.0000, accu: 0.8131, speed: 8.92 step/s\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.4481, ce_loss: 0.4481., kl_loss: 0.0000, accu: 0.8161, speed: 9.71 step/s\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.3517, ce_loss: 0.3517., kl_loss: 0.0000, accu: 0.8159, speed: 10.25 step/s\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.4341, ce_loss: 0.4341., kl_loss: 0.0000, accu: 0.8158, speed: 10.76 step/s\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.4970, ce_loss: 0.4970., kl_loss: 0.0000, accu: 0.8177, speed: 10.48 step/s\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.2409, ce_loss: 0.2409., kl_loss: 0.0000, accu: 0.8189, speed: 9.52 step/s\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.3609, ce_loss: 0.3609., kl_loss: 0.0000, accu: 0.8165, speed: 10.24 step/s\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.4755, ce_loss: 0.4755., kl_loss: 0.0000, accu: 0.8135, speed: 10.39 step/s\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.4339, ce_loss: 0.4339., kl_loss: 0.0000, accu: 0.8135, speed: 10.85 step/s\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.1610, ce_loss: 0.1610., kl_loss: 0.0000, accu: 0.8153, speed: 10.87 step/s\n",
      "dev_loss: 0.46685, accuracy: 0.79904, total_num:28802\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.3003, ce_loss: 0.3003., kl_loss: 0.0000, accu: 0.8063, speed: 0.29 step/s\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.5100, ce_loss: 0.5100., kl_loss: 0.0000, accu: 0.8125, speed: 10.04 step/s\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.4238, ce_loss: 0.4238., kl_loss: 0.0000, accu: 0.8073, speed: 9.29 step/s\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.4584, ce_loss: 0.4584., kl_loss: 0.0000, accu: 0.8039, speed: 9.50 step/s\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.4928, ce_loss: 0.4928., kl_loss: 0.0000, accu: 0.8100, speed: 9.81 step/s\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.5249, ce_loss: 0.5249., kl_loss: 0.0000, accu: 0.8089, speed: 9.18 step/s\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.3683, ce_loss: 0.3683., kl_loss: 0.0000, accu: 0.8134, speed: 9.48 step/s\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.4050, ce_loss: 0.4050., kl_loss: 0.0000, accu: 0.8148, speed: 10.26 step/s\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.3601, ce_loss: 0.3601., kl_loss: 0.0000, accu: 0.8181, speed: 9.06 step/s\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.3862, ce_loss: 0.3862., kl_loss: 0.0000, accu: 0.8181, speed: 9.04 step/s\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.2692, ce_loss: 0.2692., kl_loss: 0.0000, accu: 0.8173, speed: 9.26 step/s\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.3789, ce_loss: 0.3789., kl_loss: 0.0000, accu: 0.8148, speed: 10.13 step/s\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.3960, ce_loss: 0.3960., kl_loss: 0.0000, accu: 0.8142, speed: 10.17 step/s\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.2440, ce_loss: 0.2440., kl_loss: 0.0000, accu: 0.8141, speed: 9.27 step/s\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.2930, ce_loss: 0.2930., kl_loss: 0.0000, accu: 0.8125, speed: 9.56 step/s\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.5274, ce_loss: 0.5274., kl_loss: 0.0000, accu: 0.8117, speed: 9.69 step/s\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.2861, ce_loss: 0.2861., kl_loss: 0.0000, accu: 0.8121, speed: 9.76 step/s\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.5136, ce_loss: 0.5136., kl_loss: 0.0000, accu: 0.8116, speed: 10.16 step/s\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.3051, ce_loss: 0.3051., kl_loss: 0.0000, accu: 0.8109, speed: 9.25 step/s\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.4359, ce_loss: 0.4359., kl_loss: 0.0000, accu: 0.8087, speed: 10.61 step/s\n",
      "dev_loss: 0.45168, accuracy: 0.79932, total_num:28802\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.3014, ce_loss: 0.3014., kl_loss: 0.0000, accu: 0.8250, speed: 0.29 step/s\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.3822, ce_loss: 0.3822., kl_loss: 0.0000, accu: 0.7969, speed: 8.75 step/s\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.3486, ce_loss: 0.3486., kl_loss: 0.0000, accu: 0.8104, speed: 9.56 step/s\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.2309, ce_loss: 0.2309., kl_loss: 0.0000, accu: 0.8156, speed: 10.27 step/s\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.4536, ce_loss: 0.4536., kl_loss: 0.0000, accu: 0.8169, speed: 10.52 step/s\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.3533, ce_loss: 0.3533., kl_loss: 0.0000, accu: 0.8187, speed: 9.78 step/s\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.4743, ce_loss: 0.4743., kl_loss: 0.0000, accu: 0.8183, speed: 9.74 step/s\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.6310, ce_loss: 0.6310., kl_loss: 0.0000, accu: 0.8129, speed: 9.82 step/s\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.3286, ce_loss: 0.3286., kl_loss: 0.0000, accu: 0.8146, speed: 9.69 step/s\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.4149, ce_loss: 0.4149., kl_loss: 0.0000, accu: 0.8137, speed: 11.42 step/s\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.4488, ce_loss: 0.4488., kl_loss: 0.0000, accu: 0.8145, speed: 9.71 step/s\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.4560, ce_loss: 0.4560., kl_loss: 0.0000, accu: 0.8115, speed: 9.98 step/s\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.3099, ce_loss: 0.3099., kl_loss: 0.0000, accu: 0.8135, speed: 8.96 step/s\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.4703, ce_loss: 0.4703., kl_loss: 0.0000, accu: 0.8156, speed: 9.98 step/s\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.3478, ce_loss: 0.3478., kl_loss: 0.0000, accu: 0.8152, speed: 9.79 step/s\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.2881, ce_loss: 0.2881., kl_loss: 0.0000, accu: 0.8162, speed: 9.85 step/s\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.4028, ce_loss: 0.4028., kl_loss: 0.0000, accu: 0.8171, speed: 9.26 step/s\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.4997, ce_loss: 0.4997., kl_loss: 0.0000, accu: 0.8168, speed: 10.30 step/s\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.2682, ce_loss: 0.2682., kl_loss: 0.0000, accu: 0.8192, speed: 10.49 step/s\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.4590, ce_loss: 0.4590., kl_loss: 0.0000, accu: 0.8203, speed: 10.32 step/s\n",
      "dev_loss: 0.48376, accuracy: 0.80418, total_num:28802\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.5222, ce_loss: 0.5222., kl_loss: 0.0000, accu: 0.8406, speed: 0.29 step/s\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.4215, ce_loss: 0.4215., kl_loss: 0.0000, accu: 0.8031, speed: 11.44 step/s\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.2679, ce_loss: 0.2679., kl_loss: 0.0000, accu: 0.7958, speed: 10.72 step/s\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.6016, ce_loss: 0.6016., kl_loss: 0.0000, accu: 0.8039, speed: 10.23 step/s\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.2663, ce_loss: 0.2663., kl_loss: 0.0000, accu: 0.8137, speed: 10.34 step/s\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.2803, ce_loss: 0.2803., kl_loss: 0.0000, accu: 0.8146, speed: 10.10 step/s\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.2880, ce_loss: 0.2880., kl_loss: 0.0000, accu: 0.8138, speed: 9.44 step/s\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.3339, ce_loss: 0.3339., kl_loss: 0.0000, accu: 0.8152, speed: 9.43 step/s\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.2948, ce_loss: 0.2948., kl_loss: 0.0000, accu: 0.8187, speed: 10.43 step/s\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.4863, ce_loss: 0.4863., kl_loss: 0.0000, accu: 0.8175, speed: 10.07 step/s\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.3895, ce_loss: 0.3895., kl_loss: 0.0000, accu: 0.8159, speed: 11.14 step/s\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.5149, ce_loss: 0.5149., kl_loss: 0.0000, accu: 0.8182, speed: 9.34 step/s\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.2004, ce_loss: 0.2004., kl_loss: 0.0000, accu: 0.8202, speed: 9.34 step/s\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.5502, ce_loss: 0.5502., kl_loss: 0.0000, accu: 0.8210, speed: 9.39 step/s\n",
      "global step 1550, epoch: 1, batch: 1550, loss: 0.4881, ce_loss: 0.4881., kl_loss: 0.0000, accu: 0.8200, speed: 10.02 step/s\n",
      "global step 1560, epoch: 1, batch: 1560, loss: 0.4176, ce_loss: 0.4176., kl_loss: 0.0000, accu: 0.8195, speed: 10.02 step/s\n",
      "global step 1570, epoch: 1, batch: 1570, loss: 0.4090, ce_loss: 0.4090., kl_loss: 0.0000, accu: 0.8200, speed: 10.57 step/s\n",
      "global step 1580, epoch: 1, batch: 1580, loss: 0.2286, ce_loss: 0.2286., kl_loss: 0.0000, accu: 0.8219, speed: 10.71 step/s\n",
      "global step 1590, epoch: 1, batch: 1590, loss: 0.2268, ce_loss: 0.2268., kl_loss: 0.0000, accu: 0.8217, speed: 9.06 step/s\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.3945, ce_loss: 0.3945., kl_loss: 0.0000, accu: 0.8230, speed: 10.10 step/s\n",
      "dev_loss: 0.42537, accuracy: 0.80835, total_num:28802\n",
      "global step 1610, epoch: 1, batch: 1610, loss: 0.4064, ce_loss: 0.4064., kl_loss: 0.0000, accu: 0.8406, speed: 0.29 step/s\n",
      "global step 1620, epoch: 1, batch: 1620, loss: 0.5278, ce_loss: 0.5278., kl_loss: 0.0000, accu: 0.8406, speed: 9.48 step/s\n",
      "global step 1630, epoch: 1, batch: 1630, loss: 0.3832, ce_loss: 0.3832., kl_loss: 0.0000, accu: 0.8406, speed: 10.17 step/s\n",
      "global step 1640, epoch: 1, batch: 1640, loss: 0.4317, ce_loss: 0.4317., kl_loss: 0.0000, accu: 0.8375, speed: 10.13 step/s\n",
      "global step 1650, epoch: 1, batch: 1650, loss: 0.3482, ce_loss: 0.3482., kl_loss: 0.0000, accu: 0.8400, speed: 10.38 step/s\n",
      "global step 1660, epoch: 1, batch: 1660, loss: 0.5504, ce_loss: 0.5504., kl_loss: 0.0000, accu: 0.8385, speed: 9.82 step/s\n",
      "global step 1670, epoch: 1, batch: 1670, loss: 0.4337, ce_loss: 0.4337., kl_loss: 0.0000, accu: 0.8433, speed: 9.51 step/s\n",
      "global step 1680, epoch: 1, batch: 1680, loss: 0.4354, ce_loss: 0.4354., kl_loss: 0.0000, accu: 0.8402, speed: 9.44 step/s\n",
      "global step 1690, epoch: 1, batch: 1690, loss: 0.4566, ce_loss: 0.4566., kl_loss: 0.0000, accu: 0.8417, speed: 11.29 step/s\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.4430, ce_loss: 0.4430., kl_loss: 0.0000, accu: 0.8406, speed: 9.84 step/s\n",
      "global step 1710, epoch: 1, batch: 1710, loss: 0.7899, ce_loss: 0.7899., kl_loss: 0.0000, accu: 0.8375, speed: 8.68 step/s\n",
      "global step 1720, epoch: 1, batch: 1720, loss: 0.3385, ce_loss: 0.3385., kl_loss: 0.0000, accu: 0.8388, speed: 9.74 step/s\n",
      "global step 1730, epoch: 1, batch: 1730, loss: 0.2568, ce_loss: 0.2568., kl_loss: 0.0000, accu: 0.8358, speed: 10.74 step/s\n",
      "global step 1740, epoch: 1, batch: 1740, loss: 0.3859, ce_loss: 0.3859., kl_loss: 0.0000, accu: 0.8355, speed: 9.25 step/s\n",
      "global step 1750, epoch: 1, batch: 1750, loss: 0.2457, ce_loss: 0.2457., kl_loss: 0.0000, accu: 0.8369, speed: 10.28 step/s\n",
      "global step 1760, epoch: 1, batch: 1760, loss: 0.4325, ce_loss: 0.4325., kl_loss: 0.0000, accu: 0.8363, speed: 9.46 step/s\n",
      "global step 1770, epoch: 1, batch: 1770, loss: 0.4933, ce_loss: 0.4933., kl_loss: 0.0000, accu: 0.8344, speed: 10.35 step/s\n",
      "global step 1780, epoch: 1, batch: 1780, loss: 0.3413, ce_loss: 0.3413., kl_loss: 0.0000, accu: 0.8349, speed: 8.69 step/s\n",
      "global step 1790, epoch: 1, batch: 1790, loss: 0.2885, ce_loss: 0.2885., kl_loss: 0.0000, accu: 0.8332, speed: 9.04 step/s\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.2213, ce_loss: 0.2213., kl_loss: 0.0000, accu: 0.8320, speed: 11.50 step/s\n",
      "dev_loss: 0.44297, accuracy: 0.80845, total_num:28802\n",
      "global step 1810, epoch: 1, batch: 1810, loss: 0.4833, ce_loss: 0.4833., kl_loss: 0.0000, accu: 0.8219, speed: 0.29 step/s\n",
      "global step 1820, epoch: 1, batch: 1820, loss: 0.4707, ce_loss: 0.4707., kl_loss: 0.0000, accu: 0.8063, speed: 9.48 step/s\n",
      "global step 1830, epoch: 1, batch: 1830, loss: 0.2741, ce_loss: 0.2741., kl_loss: 0.0000, accu: 0.8094, speed: 10.28 step/s\n",
      "global step 1840, epoch: 1, batch: 1840, loss: 0.2589, ce_loss: 0.2589., kl_loss: 0.0000, accu: 0.8211, speed: 9.90 step/s\n",
      "global step 1850, epoch: 1, batch: 1850, loss: 0.3909, ce_loss: 0.3909., kl_loss: 0.0000, accu: 0.8213, speed: 10.38 step/s\n",
      "global step 1860, epoch: 1, batch: 1860, loss: 0.2645, ce_loss: 0.2645., kl_loss: 0.0000, accu: 0.8260, speed: 10.13 step/s\n",
      "global step 1870, epoch: 1, batch: 1870, loss: 0.3965, ce_loss: 0.3965., kl_loss: 0.0000, accu: 0.8254, speed: 10.03 step/s\n",
      "global step 1880, epoch: 1, batch: 1880, loss: 0.1371, ce_loss: 0.1371., kl_loss: 0.0000, accu: 0.8273, speed: 10.45 step/s\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "best_accuracy = 0.0\n",
    "\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, 3 + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits1, kl_loss = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        correct = metric.compute(logits1, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        ce_loss = criterion(logits1, labels)\n",
    "        if kl_loss > 0:\n",
    "            loss = ce_loss + kl_loss * args.rdrop_coef\n",
    "        else:\n",
    "            loss = ce_loss\n",
    "            \n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.4f, ce_loss: %.4f., kl_loss: %.4f, accu: %.4f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, ce_loss, kl_loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 200 == 0:\n",
    "            accuracy = evaluate(model, criterion, metric, dev_data_loader)\n",
    "            if accuracy > best_accuracy:\n",
    "                save_dir = os.path.join(\"./checkpoint\", \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                tokenizer.save_pretrained(save_dir)\n",
    "                best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.5 æ¨¡å‹é¢„æµ‹\n",
    "\n",
    "æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹å¯¹ä¸€äº›é¢„æµ‹æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! wget https://paddlenlp.bj.bcebos.com/models/text_matching/question_matching_rdrop0p0_baseline_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! tar -xvf question_matching_rdrop0p0_baseline_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -3 \"data/data104941/test_A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$ unset CUDA_VISIBLE_DEVICES\n",
    "!python -u \\\n",
    "    work/predict.py \\\n",
    "    --device gpu \\\n",
    "    --params_path \"./ernie_gram_rdrop0p0/model_state.pdparams\" \\\n",
    "    --batch_size 128 \\\n",
    "    --input_file \"data/data104941/test_A\" \\\n",
    "    --result_file \"ccf_qianyan_qm_result_A.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### æäº¤é¢„æµ‹ç»“æœ[2021 CCF BDCI åƒè¨€-é—®é¢˜åŒ¹é…é²æ£’æ€§è¯„æµ‹æ¯”èµ›ğŸ‘ˆ](https://aistudio.baidu.com/aistudio/competition/detail/116/0/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### æ›´å¤šä¼˜åŒ–æ–¹æ¡ˆ\n",
    "\n",
    "- å¯¹æŠ—è®­ç»ƒ\n",
    "\n",
    "- æ•°æ®å¢å¼º\n",
    "\n",
    "- å¤§æ¨¡å‹\n",
    "\n",
    "- æ¨¡å‹é›†æˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
